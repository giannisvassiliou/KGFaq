
Love-at-First-Sight:First-Sight Summaries for Rapid Query Answering in Big Knowledge Graphs
The increasing number of large knowledge graphs now available online requires methods for their effective and efficient exploration. Most of these knowledge graphs offer online SPARQL endpoints for querying and exploring their data. In a typical scenario, the users issue coarse exploratory queries at the beginning, refining them further in the sequel in order to find the answer to the question in mind. However, those coarse exploratory queries are costly to evaluate as they usually involve many results and take too much time to be answered, or even worse they time out, limiting the exploration potential of the data they expose. In this paper, we present the LFS (Love-at-First-Sight) system, offering a unique solution to the aforementioned problem, enabling users to get the first answers to their queries rapidly. More specifically we are the first to define the problem of constructing a firstsight summary, i.e., a summary able to provide rapidly the first answers to user queries. We provide effective algorithms for their construction, relying on existing query logs, and we both analytically and experimentally show the big benefits of the proposed summaries. They have a compact size, and they can efficiently provide first answers (even to unseen) user queries, improving by orders of magnitude query response times

Sublime's custom image

LFS Data Creator
Firstly we need to create the data by accessing the endpoint of the desired dataset. The parserIT.py script has the following syntax

USAGE: parserIT queryfile {flag 1/0 m(ost) f(requent) or non mf} {basefilename} {limit} {urlendpoint}

Where

queryfile : The name of the original query log (see data folder, choose e.g YAGO_orig_quer.txt)
flag : 0 or 1 whether we need most frequent results 1 (yes) or 0 (no)
basefilename: a string to base the output file names {e.g yago}
limit: a SPARQL limit {e.g. 500}
urlendpoint: a valid url endpoint ( e.g. https://yago-knowledge.org/sparql/query )

We have stored in data folder results-examples for the 3 datasets we used (DBpedia, YAGO, Wikidata)
These files can be used directly from the LFS Evaluator



LFS Evaluator
You need to provide two INPUT files ( orig_summary_filename and queries_for_summary ) and one filename for OUTPUT (the actual .nt LFS Summary ) , finally the address_of_endpoint {OPTIONAL}

USAGE: lfs orig_summary_filename queries_for_summary LFS_summary_output {url of endpoint - optional}

Where

orig_summary_filename: The filename of the summary that parserIT produced
queries_for_summary: The filename of the previous summary, corresponding queries
LFS_summary_output: The final .nt file of the actual LFS summary
address_of_endpoint: if given, the system will try to evaluate the queries cannot be answered by the LFS Summary, from the endpoint

The previous script, will:

Create the train/test portions from the orig_summary_filename
Create the lfs .nt summary (from the train portion)
Query the .nt summary created with the test queries
Present the % of the first-sight queries replied, and the time consumed
Used Python v3.9 - Required Python libraries

rdflib
pandas
SPARQLWrapper
numpy
sys
JSON
